{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Model: LSTMpred(\n",
      "  (lstm): LSTM(1, 16)\n",
      "  (hidden2out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "2019-07-16 13:22:23 Epoch: 0 Loss: 0.3798284\n",
      "2019-07-16 13:22:26 Epoch: 1 Loss: 0.32666743\n",
      "2019-07-16 13:22:28 Epoch: 2 Loss: 0.3060625\n",
      "2019-07-16 13:22:29 Epoch: 3 Loss: 0.288956\n",
      "2019-07-16 13:22:31 Epoch: 4 Loss: 0.27155545\n",
      "2019-07-16 13:22:31 Epoch: 5 Loss: 0.25329092\n",
      "2019-07-16 13:22:33 Epoch: 6 Loss: 0.23406036\n",
      "2019-07-16 13:22:36 Epoch: 7 Loss: 0.21388765\n",
      "2019-07-16 13:22:39 Epoch: 8 Loss: 0.1928892\n",
      "2019-07-16 13:22:41 Epoch: 9 Loss: 0.17128545\n",
      "2019-07-16 13:22:44 Epoch: 10 Loss: 0.1494087\n",
      "2019-07-16 13:22:45 Epoch: 11 Loss: 0.12770358\n",
      "2019-07-16 13:22:46 Epoch: 12 Loss: 0.10670421\n",
      "2019-07-16 13:22:48 Epoch: 13 Loss: 0.086988054\n",
      "2019-07-16 13:22:51 Epoch: 14 Loss: 0.06910469\n",
      "2019-07-16 13:22:52 Epoch: 15 Loss: 0.053490147\n",
      "2019-07-16 13:22:53 Epoch: 16 Loss: 0.040395264\n",
      "2019-07-16 13:22:55 Epoch: 17 Loss: 0.02985012\n",
      "2019-07-16 13:22:58 Epoch: 18 Loss: 0.021681\n",
      "2019-07-16 13:23:00 Epoch: 19 Loss: 0.01556871\n",
      "2019-07-16 13:23:02 Epoch: 20 Loss: 0.01112704\n",
      "2019-07-16 13:23:03 Epoch: 21 Loss: 0.007971484\n",
      "2019-07-16 13:23:04 Epoch: 22 Loss: 0.005764351\n",
      "2019-07-16 13:23:05 Epoch: 23 Loss: 0.0042341556\n",
      "2019-07-16 13:23:07 Epoch: 24 Loss: 0.0031762554\n",
      "2019-07-16 13:23:09 Epoch: 25 Loss: 0.0024430964\n",
      "2019-07-16 13:23:11 Epoch: 26 Loss: 0.0019317458\n",
      "2019-07-16 13:23:13 Epoch: 27 Loss: 0.0015717415\n",
      "2019-07-16 13:23:15 Epoch: 28 Loss: 0.0013154583\n",
      "2019-07-16 13:23:17 Epoch: 29 Loss: 0.0011307253\n",
      "2019-07-16 13:23:19 Epoch: 30 Loss: 0.0009958834\n",
      "2019-07-16 13:23:21 Epoch: 31 Loss: 0.0008962293\n",
      "2019-07-16 13:23:23 Epoch: 32 Loss: 0.0008216416\n",
      "2019-07-16 13:23:25 Epoch: 33 Loss: 0.00076510594\n",
      "2019-07-16 13:23:27 Epoch: 34 Loss: 0.00072175096\n",
      "2019-07-16 13:23:29 Epoch: 35 Loss: 0.0006880774\n",
      "2019-07-16 13:23:31 Epoch: 36 Loss: 0.000661546\n",
      "2019-07-16 13:23:33 Epoch: 37 Loss: 0.0006403658\n",
      "2019-07-16 13:23:35 Epoch: 38 Loss: 0.0006231911\n",
      "2019-07-16 13:23:38 Epoch: 39 Loss: 0.00060905033\n",
      "2019-07-16 13:23:40 Epoch: 40 Loss: 0.00059719663\n",
      "2019-07-16 13:23:43 Epoch: 41 Loss: 0.00058709015\n",
      "2019-07-16 13:23:44 Epoch: 42 Loss: 0.0005783078\n",
      "2019-07-16 13:23:45 Epoch: 43 Loss: 0.00057055074\n",
      "2019-07-16 13:23:47 Epoch: 44 Loss: 0.0005635873\n",
      "2019-07-16 13:23:50 Epoch: 45 Loss: 0.0005572349\n",
      "2019-07-16 13:23:52 Epoch: 46 Loss: 0.0005513663\n",
      "2019-07-16 13:23:53 Epoch: 47 Loss: 0.00054585456\n",
      "2019-07-16 13:23:55 Epoch: 48 Loss: 0.0005406421\n",
      "2019-07-16 13:23:58 Epoch: 49 Loss: 0.00053566985\n",
      "2019-07-16 13:23:59 Epoch: 50 Loss: 0.0005308826\n",
      "2019-07-16 13:24:02 Epoch: 51 Loss: 0.0005262536\n",
      "2019-07-16 13:24:04 Epoch: 52 Loss: 0.0005217347\n",
      "2019-07-16 13:24:05 Epoch: 53 Loss: 0.0005173465\n",
      "2019-07-16 13:24:06 Epoch: 54 Loss: 0.00051304157\n",
      "2019-07-16 13:24:08 Epoch: 55 Loss: 0.00050881377\n",
      "2019-07-16 13:24:10 Epoch: 56 Loss: 0.0005046624\n",
      "2019-07-16 13:24:11 Epoch: 57 Loss: 0.0005005494\n",
      "2019-07-16 13:24:11 Epoch: 58 Loss: 0.00049652223\n",
      "2019-07-16 13:24:13 Epoch: 59 Loss: 0.0004925299\n",
      "2019-07-16 13:24:15 Epoch: 60 Loss: 0.0004885985\n",
      "2019-07-16 13:24:16 Epoch: 61 Loss: 0.00048470902\n",
      "2019-07-16 13:24:18 Epoch: 62 Loss: 0.00048086388\n",
      "2019-07-16 13:24:21 Epoch: 63 Loss: 0.00047705226\n",
      "2019-07-16 13:24:23 Epoch: 64 Loss: 0.00047328175\n",
      "2019-07-16 13:24:26 Epoch: 65 Loss: 0.00046956495\n",
      "2019-07-16 13:24:28 Epoch: 66 Loss: 0.00046588853\n",
      "2019-07-16 13:24:30 Epoch: 67 Loss: 0.0004622317\n",
      "2019-07-16 13:24:32 Epoch: 68 Loss: 0.00045863265\n",
      "2019-07-16 13:24:34 Epoch: 69 Loss: 0.00045505786\n",
      "2019-07-16 13:24:37 Epoch: 70 Loss: 0.00045150463\n",
      "2019-07-16 13:24:38 Epoch: 71 Loss: 0.00044800068\n",
      "2019-07-16 13:24:39 Epoch: 72 Loss: 0.00044453048\n",
      "2019-07-16 13:24:41 Epoch: 73 Loss: 0.00044108878\n",
      "2019-07-16 13:24:44 Epoch: 74 Loss: 0.00043768043\n",
      "2019-07-16 13:24:46 Epoch: 75 Loss: 0.00043431757\n",
      "2019-07-16 13:24:49 Epoch: 76 Loss: 0.00043097264\n",
      "2019-07-16 13:24:51 Epoch: 77 Loss: 0.00042766283\n",
      "2019-07-16 13:24:52 Epoch: 78 Loss: 0.00042438542\n",
      "2019-07-16 13:24:52 Epoch: 79 Loss: 0.0004211402\n",
      "2019-07-16 13:24:53 Epoch: 80 Loss: 0.00041791715\n",
      "2019-07-16 13:24:55 Epoch: 81 Loss: 0.0004147405\n",
      "2019-07-16 13:24:57 Epoch: 82 Loss: 0.0004115953\n",
      "2019-07-16 13:24:59 Epoch: 83 Loss: 0.0004084669\n",
      "2019-07-16 13:25:00 Epoch: 84 Loss: 0.00040537922\n",
      "2019-07-16 13:25:01 Epoch: 85 Loss: 0.00040231762\n",
      "2019-07-16 13:25:02 Epoch: 86 Loss: 0.0003992962\n",
      "2019-07-16 13:25:03 Epoch: 87 Loss: 0.00039628855\n",
      "2019-07-16 13:25:06 Epoch: 88 Loss: 0.00039330882\n",
      "2019-07-16 13:25:09 Epoch: 89 Loss: 0.00039035446\n",
      "2019-07-16 13:25:10 Epoch: 90 Loss: 0.0003874394\n",
      "2019-07-16 13:25:12 Epoch: 91 Loss: 0.00038455395\n",
      "2019-07-16 13:25:13 Epoch: 92 Loss: 0.00038169094\n",
      "2019-07-16 13:25:14 Epoch: 93 Loss: 0.0003788456\n",
      "2019-07-16 13:25:14 Epoch: 94 Loss: 0.000376034\n",
      "2019-07-16 13:25:15 Epoch: 95 Loss: 0.00037326053\n",
      "2019-07-16 13:25:18 Epoch: 96 Loss: 0.00037049502\n",
      "2019-07-16 13:25:20 Epoch: 97 Loss: 0.00036776724\n",
      "2019-07-16 13:25:23 Epoch: 98 Loss: 0.00036505863\n",
      "2019-07-16 13:25:26 Epoch: 99 Loss: 0.0003623782\n",
      "2019-07-16 13:25:28 Epoch: 100 Loss: 0.0003597212\n",
      "2019-07-16 13:25:29 Epoch: 101 Loss: 0.00035708974\n",
      "2019-07-16 13:25:30 Epoch: 102 Loss: 0.0003544769\n",
      "2019-07-16 13:25:30 Epoch: 103 Loss: 0.00035188935\n",
      "2019-07-16 13:25:32 Epoch: 104 Loss: 0.00034933133\n",
      "2019-07-16 13:25:34 Epoch: 105 Loss: 0.00034680037\n",
      "2019-07-16 13:25:35 Epoch: 106 Loss: 0.0003442875\n",
      "2019-07-16 13:25:36 Epoch: 107 Loss: 0.00034180577\n",
      "2019-07-16 13:25:36 Epoch: 108 Loss: 0.00033933303\n",
      "2019-07-16 13:25:37 Epoch: 109 Loss: 0.00033688676\n",
      "2019-07-16 13:25:38 Epoch: 110 Loss: 0.0003344755\n",
      "2019-07-16 13:25:39 Epoch: 111 Loss: 0.0003320686\n",
      "2019-07-16 13:25:41 Epoch: 112 Loss: 0.000329692\n",
      "2019-07-16 13:25:44 Epoch: 113 Loss: 0.00032733686\n",
      "2019-07-16 13:25:46 Epoch: 114 Loss: 0.0003249945\n",
      "2019-07-16 13:25:48 Epoch: 115 Loss: 0.00032269265\n",
      "2019-07-16 13:25:50 Epoch: 116 Loss: 0.0003203947\n",
      "2019-07-16 13:25:51 Epoch: 117 Loss: 0.0003181156\n",
      "2019-07-16 13:25:54 Epoch: 118 Loss: 0.00031586585\n",
      "2019-07-16 13:25:56 Epoch: 119 Loss: 0.0003136325\n",
      "2019-07-16 13:25:57 Epoch: 120 Loss: 0.0003114197\n",
      "2019-07-16 13:25:58 Epoch: 121 Loss: 0.00030923152\n",
      "2019-07-16 13:26:00 Epoch: 122 Loss: 0.0003070573\n",
      "2019-07-16 13:26:03 Epoch: 123 Loss: 0.00030491158\n",
      "2019-07-16 13:26:05 Epoch: 124 Loss: 0.00030278374\n",
      "2019-07-16 13:26:08 Epoch: 125 Loss: 0.00030067575\n",
      "2019-07-16 13:26:09 Epoch: 126 Loss: 0.00029857925\n",
      "2019-07-16 13:26:12 Epoch: 127 Loss: 0.0002964983\n",
      "2019-07-16 13:26:13 Epoch: 128 Loss: 0.00029444305\n",
      "2019-07-16 13:26:15 Epoch: 129 Loss: 0.00029241125\n",
      "2019-07-16 13:26:17 Epoch: 130 Loss: 0.00029039255\n",
      "2019-07-16 13:26:19 Epoch: 131 Loss: 0.0002883829\n",
      "2019-07-16 13:26:21 Epoch: 132 Loss: 0.00028640038\n",
      "2019-07-16 13:26:22 Epoch: 133 Loss: 0.00028443075\n",
      "2019-07-16 13:26:24 Epoch: 134 Loss: 0.00028248594\n",
      "2019-07-16 13:26:26 Epoch: 135 Loss: 0.00028055377\n",
      "2019-07-16 13:26:28 Epoch: 136 Loss: 0.00027865014\n",
      "2019-07-16 13:26:30 Epoch: 137 Loss: 0.000276751\n",
      "2019-07-16 13:26:31 Epoch: 138 Loss: 0.00027487616\n",
      "2019-07-16 13:26:32 Epoch: 139 Loss: 0.00027300374\n",
      "2019-07-16 13:26:32 Epoch: 140 Loss: 0.0002711593\n",
      "2019-07-16 13:26:33 Epoch: 141 Loss: 0.00026933482\n",
      "2019-07-16 13:26:34 Epoch: 142 Loss: 0.0002675087\n",
      "2019-07-16 13:26:35 Epoch: 143 Loss: 0.0002657218\n",
      "2019-07-16 13:26:36 Epoch: 144 Loss: 0.00026393513\n",
      "2019-07-16 13:26:37 Epoch: 145 Loss: 0.00026216602\n",
      "2019-07-16 13:26:40 Epoch: 146 Loss: 0.00026040676\n",
      "2019-07-16 13:26:41 Epoch: 147 Loss: 0.00025867255\n",
      "2019-07-16 13:26:41 Epoch: 148 Loss: 0.00025694986\n",
      "2019-07-16 13:26:42 Epoch: 149 Loss: 0.00025524438\n",
      "2019-07-16 13:26:43 Epoch: 150 Loss: 0.00025354835\n",
      "2019-07-16 13:26:44 Epoch: 151 Loss: 0.00025186557\n",
      "2019-07-16 13:26:46 Epoch: 152 Loss: 0.0002501997\n",
      "2019-07-16 13:26:47 Epoch: 153 Loss: 0.00024855812\n",
      "2019-07-16 13:26:48 Epoch: 154 Loss: 0.00024692947\n",
      "2019-07-16 13:26:51 Epoch: 155 Loss: 0.0002453099\n",
      "2019-07-16 13:26:52 Epoch: 156 Loss: 0.00024370309\n",
      "2019-07-16 13:26:53 Epoch: 157 Loss: 0.000242109\n",
      "2019-07-16 13:26:55 Epoch: 158 Loss: 0.00024053123\n",
      "2019-07-16 13:26:56 Epoch: 159 Loss: 0.00023896966\n",
      "2019-07-16 13:26:59 Epoch: 160 Loss: 0.00023741317\n",
      "2019-07-16 13:27:01 Epoch: 161 Loss: 0.00023587277\n",
      "2019-07-16 13:27:02 Epoch: 162 Loss: 0.00023435015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-16 13:27:03 Epoch: 163 Loss: 0.0002328361\n",
      "2019-07-16 13:27:04 Epoch: 164 Loss: 0.00023133965\n",
      "2019-07-16 13:27:05 Epoch: 165 Loss: 0.00022985163\n",
      "2019-07-16 13:27:08 Epoch: 166 Loss: 0.00022838284\n",
      "2019-07-16 13:27:09 Epoch: 167 Loss: 0.00022691156\n",
      "2019-07-16 13:27:11 Epoch: 168 Loss: 0.00022545578\n",
      "2019-07-16 13:27:13 Epoch: 169 Loss: 0.00022401895\n",
      "2019-07-16 13:27:14 Epoch: 170 Loss: 0.0002225885\n",
      "2019-07-16 13:27:16 Epoch: 171 Loss: 0.00022117326\n",
      "2019-07-16 13:27:17 Epoch: 172 Loss: 0.00021977493\n",
      "2019-07-16 13:27:20 Epoch: 173 Loss: 0.00021838454\n",
      "2019-07-16 13:27:22 Epoch: 174 Loss: 0.00021700733\n",
      "2019-07-16 13:27:25 Epoch: 175 Loss: 0.00021563799\n",
      "2019-07-16 13:27:28 Epoch: 176 Loss: 0.00021429044\n",
      "2019-07-16 13:27:29 Epoch: 177 Loss: 0.0002129471\n",
      "2019-07-16 13:27:31 Epoch: 178 Loss: 0.00021162014\n",
      "2019-07-16 13:27:33 Epoch: 179 Loss: 0.00021030249\n",
      "2019-07-16 13:27:34 Epoch: 180 Loss: 0.00020899242\n",
      "2019-07-16 13:27:36 Epoch: 181 Loss: 0.00020768472\n",
      "2019-07-16 13:27:38 Epoch: 182 Loss: 0.00020640167\n",
      "2019-07-16 13:27:39 Epoch: 183 Loss: 0.00020512602\n",
      "2019-07-16 13:27:41 Epoch: 184 Loss: 0.00020386794\n",
      "2019-07-16 13:27:42 Epoch: 185 Loss: 0.00020261711\n",
      "2019-07-16 13:27:45 Epoch: 186 Loss: 0.00020136846\n",
      "2019-07-16 13:27:46 Epoch: 187 Loss: 0.00020013377\n",
      "2019-07-16 13:27:47 Epoch: 188 Loss: 0.00019891802\n",
      "2019-07-16 13:27:50 Epoch: 189 Loss: 0.0001976959\n",
      "2019-07-16 13:27:52 Epoch: 190 Loss: 0.00019649428\n",
      "2019-07-16 13:27:55 Epoch: 191 Loss: 0.00019529297\n",
      "2019-07-16 13:27:55 Epoch: 192 Loss: 0.00019409867\n",
      "2019-07-16 13:27:58 Epoch: 193 Loss: 0.00019292129\n",
      "2019-07-16 13:27:59 Epoch: 194 Loss: 0.00019175904\n",
      "2019-07-16 13:28:00 Epoch: 195 Loss: 0.000190597\n",
      "2019-07-16 13:28:02 Epoch: 196 Loss: 0.00018944999\n",
      "2019-07-16 13:28:03 Epoch: 197 Loss: 0.00018830644\n",
      "2019-07-16 13:28:04 Epoch: 198 Loss: 0.00018717613\n",
      "2019-07-16 13:28:04 Epoch: 199 Loss: 0.00018605248\n",
      "2019-07-16 13:28:06 Epoch: 200 Loss: 0.00018494681\n",
      "2019-07-16 13:28:08 Epoch: 201 Loss: 0.00018384604\n",
      "2019-07-16 13:28:09 Epoch: 202 Loss: 0.00018275178\n",
      "2019-07-16 13:28:10 Epoch: 203 Loss: 0.00018167205\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import *\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "INPUT_SIZE=1\n",
    "HIDDEN_SIZE=16\n",
    "TIME_STEP=10\n",
    "\n",
    "EPOCH=300\n",
    "TRAIN_RATE=0.8\n",
    "# OPIMIZER='Adam'\n",
    "OPIMIZER='SGD'\n",
    "def SeriesGen(N):\n",
    "    x = torch.arange(1, N, 0.01)\n",
    "#     return torch.sin(x)#+torch.cos(x)+torch.pow(x,2)\n",
    "    return torch.sin(x)*x*x+torch.cos(x)+torch.pow(x,2)\n",
    "\n",
    "\n",
    "def trainDataGen(seq, k):\n",
    "    dat = list()\n",
    "    L = len(seq)\n",
    "    for i in range(L - k - 1):\n",
    "        indat = seq[i:i + k]\n",
    "        outdat = seq[i + k :i + k + 1]\n",
    "        # print(indat)\n",
    "        # print(outdat)\n",
    "        dat.append((indat, outdat))\n",
    "    return dat\n",
    "\n",
    "'''数据归一化到0-1'''\n",
    "def normalization(y):\n",
    "    max_value = np.max(y)\n",
    "    min_value = np.min(y)\n",
    "    scalar = max_value - min_value\n",
    "    y = list(map(lambda x: x / scalar, y))\n",
    "    return  y\n",
    "\n",
    "'''产生数据'''\n",
    "DATA_SIZE=10\n",
    "y = SeriesGen(DATA_SIZE).numpy()\n",
    "y=normalization(y)\n",
    "dat = trainDataGen(y, TIME_STEP)\n",
    "\n",
    "'''训练数据和测试数据设置'''\n",
    "train_size = int(len(dat) * TRAIN_RATE)\n",
    "test_size = len(dat) - train_size\n",
    "train_y = dat[:train_size]\n",
    "test_y = dat[train_size:]\n",
    "\n",
    "\n",
    "# test_x = range(int(len(dat) * TRAIN_RATE), len(dat))\n",
    "\n",
    "\n",
    "\n",
    "class LSTMpred(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(LSTMpred, self).__init__()\n",
    "        self.input_dim = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, 1)\n",
    "    #     self.hidden = self.init_hidden()\n",
    "    #\n",
    "    # def init_hidden(self):\n",
    "    #     return (Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "    #             Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, seq):\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            seq.view(len(seq), 1, -1))\n",
    "\n",
    "        # print('seq.view(len(seq), 1, -1)', seq.view(len(seq), 1, -1).shape)\n",
    "        # print('lstm_out', lstm_out.shape)\n",
    "\n",
    "        outdat = self.hidden2out(lstm_out.view(len(seq), -1))\n",
    "\n",
    "        # print('lstm_out.view(len(seq), -1)',lstm_out.view(len(seq), -1).shape)\n",
    "        return outdat[9,:]\n",
    "\n",
    "\n",
    "def ToVariable(x):\n",
    "    tmp = torch.FloatTensor(x)\n",
    "    return Variable(tmp)\n",
    "\n",
    "\n",
    "def model_training():\n",
    "    print('Start training...')\n",
    "    t0=time.time()\n",
    "\n",
    "    model = LSTMpred(INPUT_SIZE, HIDDEN_SIZE).cuda()\n",
    "    print('Model:',model)\n",
    "    loss_function = nn.MSELoss()\n",
    "    if OPIMIZER=='Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        for seq, outs in train_y:\n",
    "            # print(outs.shape)\n",
    "            seq = ToVariable(seq).cuda()\n",
    "            outs = ToVariable(outs).cuda()\n",
    "            # outs=torch.unsqueeze(outs,1)\n",
    "            # outs = torch.from_numpy(np.array([outs]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # model.hidden = model.init_hidden()\n",
    "\n",
    "            modout = model(seq)\n",
    "\n",
    "            loss = loss_function(modout, outs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        tim1 = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(tim1,'Epoch:',epoch,'Loss:',loss.cpu().data.numpy())\n",
    "\n",
    "    t1 = time.time() - t0\n",
    "    print('Training time: ', t1)\n",
    "    torch.save(model,'model_rnn_10_1.pkl')\n",
    "\n",
    "def model_test():\n",
    "    model=torch.load('model_rnn_10_1.pkl')\n",
    "\n",
    "    predDat = []\n",
    "    for seq, trueVal in dat:\n",
    "        seq = ToVariable(seq).cuda()\n",
    "        trueVal = ToVariable(trueVal).cuda()\n",
    "        # predDat.append(model(seq)[-1].data.numpy()[0])\n",
    "        predDat.append(model(seq).cpu().data.numpy())\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.xlabel(OPIMIZER + ', Epoch=%s' % EPOCH)\n",
    "    plt.ylabel(r'Performance')\n",
    "    plt.plot(y, label='Real-sequence')\n",
    "    plt.plot(predDat, label='Predicted-sequence')\n",
    "    plt.legend(loc=2, ncol=1)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "    model_training()\n",
    "\n",
    "    model_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
