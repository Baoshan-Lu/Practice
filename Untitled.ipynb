{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Epoch: 0 loss 0.000502108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LSTMpred. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss 0.0005509455\n",
      "Epoch: 2 loss 0.00061423436\n",
      "Epoch: 3 loss 0.0006961006\n",
      "Epoch: 4 loss 0.00079944124\n",
      "Epoch: 5 loss 0.00092533726\n",
      "Epoch: 6 loss 0.0010726791\n",
      "Epoch: 7 loss 0.0012381449\n",
      "Epoch: 8 loss 0.0014165454\n",
      "Epoch: 9 loss 0.0016014937\n",
      "Epoch: 10 loss 0.0017862301\n",
      "Epoch: 11 loss 0.0019644084\n",
      "Epoch: 12 loss 0.0021307035\n",
      "Epoch: 13 loss 0.0022811305\n",
      "Epoch: 14 loss 0.0024131367\n",
      "Epoch: 15 loss 0.002525485\n",
      "Epoch: 16 loss 0.0026180353\n",
      "Epoch: 17 loss 0.0026914661\n",
      "Epoch: 18 loss 0.0027470149\n",
      "Epoch: 19 loss 0.0027862433\n",
      "Epoch: 20 loss 0.0028108608\n",
      "Epoch: 21 loss 0.0028225998\n",
      "Epoch: 22 loss 0.0028231214\n",
      "Epoch: 23 loss 0.0028139595\n",
      "Epoch: 24 loss 0.0027965135\n",
      "Epoch: 25 loss 0.0027720171\n",
      "Epoch: 26 loss 0.0027415648\n",
      "Epoch: 27 loss 0.0027061005\n",
      "Epoch: 28 loss 0.0026664464\n",
      "Epoch: 29 loss 0.002623304\n",
      "Epoch: 30 loss 0.0025772767\n",
      "Epoch: 31 loss 0.0025288865\n",
      "Epoch: 32 loss 0.0024785781\n",
      "Epoch: 33 loss 0.0024267368\n",
      "Epoch: 34 loss 0.0023736944\n",
      "Epoch: 35 loss 0.0023197355\n",
      "Epoch: 36 loss 0.0022651132\n",
      "Epoch: 37 loss 0.0022100448\n",
      "Epoch: 38 loss 0.0021547219\n",
      "Epoch: 39 loss 0.002099319\n",
      "Epoch: 40 loss 0.0020439832\n",
      "Epoch: 41 loss 0.0019888468\n",
      "Epoch: 42 loss 0.0019340243\n",
      "Epoch: 43 loss 0.0018796284\n",
      "Epoch: 44 loss 0.0018257527\n",
      "Epoch: 45 loss 0.0017724825\n",
      "Epoch: 46 loss 0.001719896\n",
      "Epoch: 47 loss 0.0016680611\n",
      "Epoch: 48 loss 0.0016170414\n",
      "Epoch: 49 loss 0.0015668885\n",
      "Epoch: 50 loss 0.0015176582\n",
      "Epoch: 51 loss 0.0014693926\n",
      "Epoch: 52 loss 0.0014221292\n",
      "Epoch: 53 loss 0.0013759037\n",
      "Epoch: 54 loss 0.001330741\n",
      "Epoch: 55 loss 0.0012866711\n",
      "Epoch: 56 loss 0.0012437104\n",
      "Epoch: 57 loss 0.0012018766\n",
      "Epoch: 58 loss 0.0011611788\n",
      "Epoch: 59 loss 0.0011216316\n",
      "Epoch: 60 loss 0.0010832343\n",
      "Epoch: 61 loss 0.0010459934\n",
      "Epoch: 62 loss 0.0010099079\n",
      "Epoch: 63 loss 0.0009749703\n",
      "Epoch: 64 loss 0.0009411738\n",
      "Epoch: 65 loss 0.0009085087\n",
      "Epoch: 66 loss 0.00087696436\n",
      "Epoch: 67 loss 0.0008465269\n",
      "Epoch: 68 loss 0.00081717625\n",
      "Epoch: 69 loss 0.0007888996\n",
      "Epoch: 70 loss 0.00076167134\n",
      "Epoch: 71 loss 0.000735474\n",
      "Epoch: 72 loss 0.0007102831\n",
      "Epoch: 73 loss 0.00068607053\n",
      "Epoch: 74 loss 0.00066281814\n",
      "Epoch: 75 loss 0.00064049294\n",
      "Epoch: 76 loss 0.00061907107\n",
      "Epoch: 77 loss 0.00059852796\n",
      "Epoch: 78 loss 0.0005788315\n",
      "Epoch: 79 loss 0.0005599543\n",
      "Epoch: 80 loss 0.00054186804\n",
      "Epoch: 81 loss 0.0005245466\n",
      "Epoch: 82 loss 0.00050795946\n",
      "Epoch: 83 loss 0.0004920788\n",
      "Epoch: 84 loss 0.0004768783\n",
      "Epoch: 85 loss 0.00046232977\n",
      "Epoch: 86 loss 0.00044840606\n",
      "Epoch: 87 loss 0.0004350827\n",
      "Epoch: 88 loss 0.0004223316\n",
      "Epoch: 89 loss 0.00041012885\n",
      "Epoch: 90 loss 0.00039845012\n",
      "Epoch: 91 loss 0.00038727085\n",
      "Epoch: 92 loss 0.00037656765\n",
      "Epoch: 93 loss 0.00036632136\n",
      "Epoch: 94 loss 0.0003565074\n",
      "Epoch: 95 loss 0.00034710634\n",
      "Epoch: 96 loss 0.00033809902\n",
      "Epoch: 97 loss 0.00032946587\n",
      "Epoch: 98 loss 0.0003211865\n",
      "Epoch: 99 loss 0.00031324648\n",
      "Epoch: 100 loss 0.00030562753\n",
      "Epoch: 101 loss 0.0002983143\n",
      "Epoch: 102 loss 0.00029129087\n",
      "Epoch: 103 loss 0.00028454393\n",
      "Epoch: 104 loss 0.00027805995\n",
      "Epoch: 105 loss 0.00027182253\n",
      "Epoch: 106 loss 0.00026582368\n",
      "Epoch: 107 loss 0.00026004866\n",
      "Epoch: 108 loss 0.00025448788\n",
      "Epoch: 109 loss 0.00024913036\n",
      "Epoch: 110 loss 0.00024396565\n",
      "Epoch: 111 loss 0.00023898398\n",
      "Epoch: 112 loss 0.00023417678\n",
      "Epoch: 113 loss 0.00022953488\n",
      "Epoch: 114 loss 0.00022505145\n",
      "Epoch: 115 loss 0.00022071782\n",
      "Epoch: 116 loss 0.00021652764\n",
      "Epoch: 117 loss 0.00021247352\n",
      "Epoch: 118 loss 0.00020854834\n",
      "Epoch: 119 loss 0.00020474798\n",
      "Epoch: 120 loss 0.00020106563\n",
      "Epoch: 121 loss 0.00019749414\n",
      "Epoch: 122 loss 0.000194032\n",
      "Epoch: 123 loss 0.00019067098\n",
      "Epoch: 124 loss 0.00018740888\n",
      "Epoch: 125 loss 0.00018423962\n",
      "Epoch: 126 loss 0.00018115992\n",
      "Epoch: 127 loss 0.0001781673\n",
      "Epoch: 128 loss 0.00017525557\n",
      "Epoch: 129 loss 0.00017242343\n",
      "Epoch: 130 loss 0.00016966564\n",
      "Epoch: 131 loss 0.00016698118\n",
      "Epoch: 132 loss 0.00016436607\n",
      "Epoch: 133 loss 0.0001618166\n",
      "Epoch: 134 loss 0.0001593317\n",
      "Epoch: 135 loss 0.00015690849\n",
      "Epoch: 136 loss 0.00015454376\n",
      "Epoch: 137 loss 0.000152237\n",
      "Epoch: 138 loss 0.000149985\n",
      "Epoch: 139 loss 0.00014778505\n",
      "Epoch: 140 loss 0.00014563749\n",
      "Epoch: 141 loss 0.00014353779\n",
      "Epoch: 142 loss 0.00014148536\n",
      "Epoch: 143 loss 0.00013947905\n",
      "Epoch: 144 loss 0.00013751762\n",
      "Epoch: 145 loss 0.00013559895\n",
      "Epoch: 146 loss 0.0001337201\n",
      "Epoch: 147 loss 0.00013188261\n",
      "Epoch: 148 loss 0.00013008353\n",
      "Epoch: 149 loss 0.00012832107\n",
      "Epoch: 150 loss 0.00012659554\n",
      "Epoch: 151 loss 0.00012490503\n",
      "Epoch: 152 loss 0.00012324889\n",
      "Epoch: 153 loss 0.00012162477\n",
      "Epoch: 154 loss 0.00012003347\n",
      "Epoch: 155 loss 0.00011847345\n",
      "Epoch: 156 loss 0.00011694324\n",
      "Epoch: 157 loss 0.00011544248\n",
      "Epoch: 158 loss 0.00011396974\n",
      "Epoch: 159 loss 0.00011252528\n",
      "Epoch: 160 loss 0.00011110785\n",
      "Epoch: 161 loss 0.000109716726\n",
      "Epoch: 162 loss 0.0001083509\n",
      "Epoch: 163 loss 0.000107010026\n",
      "Epoch: 164 loss 0.00010569227\n",
      "Epoch: 165 loss 0.000104399536\n",
      "Epoch: 166 loss 0.00010312875\n",
      "Epoch: 167 loss 0.00010188033\n",
      "Epoch: 168 loss 0.000100653655\n",
      "Epoch: 169 loss 9.944841e-05\n",
      "Epoch: 170 loss 9.826443e-05\n",
      "Epoch: 171 loss 9.710029e-05\n",
      "Epoch: 172 loss 9.595491e-05\n",
      "Epoch: 173 loss 9.4830015e-05\n",
      "Epoch: 174 loss 9.372318e-05\n",
      "Epoch: 175 loss 9.263447e-05\n",
      "Epoch: 176 loss 9.156403e-05\n",
      "Epoch: 177 loss 9.0511596e-05\n",
      "Epoch: 178 loss 8.94751e-05\n",
      "Epoch: 179 loss 8.845598e-05\n",
      "Epoch: 180 loss 8.7452994e-05\n",
      "Epoch: 181 loss 8.646602e-05\n",
      "Epoch: 182 loss 8.549438e-05\n",
      "Epoch: 183 loss 8.453829e-05\n",
      "Epoch: 184 loss 8.35969e-05\n",
      "Epoch: 185 loss 8.2670216e-05\n",
      "Epoch: 186 loss 8.1758146e-05\n",
      "Epoch: 187 loss 8.085965e-05\n",
      "Epoch: 188 loss 7.997503e-05\n",
      "Epoch: 189 loss 7.9102916e-05\n",
      "Epoch: 190 loss 7.824568e-05\n",
      "Epoch: 191 loss 7.739987e-05\n",
      "Epoch: 192 loss 7.656694e-05\n",
      "Epoch: 193 loss 7.5746626e-05\n",
      "Epoch: 194 loss 7.493839e-05\n",
      "Epoch: 195 loss 7.414147e-05\n",
      "Epoch: 196 loss 7.33573e-05\n",
      "Epoch: 197 loss 7.258306e-05\n",
      "Epoch: 198 loss 7.182064e-05\n",
      "Epoch: 199 loss 7.1069175e-05\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8d277bbf3347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mmodel_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8d277bbf3347>\u001b[0m in \u001b[0;36mmodel_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mtrueVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrueVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mpredDat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8d277bbf3347>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         lstm_out, self.hidden = self.lstm(\n\u001b[0;32m---> 72\u001b[0;31m             seq.view(len(seq), 1, -1))\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0moutdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutdat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 522\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import *\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "INPUT_SIZE=1\n",
    "HIDDEN_SIZE=16\n",
    "TIME_STEP=10\n",
    "\n",
    "EPOCH=200\n",
    "TRAIN_RATE=0.8\n",
    "# OPIMIZER='Adam'\n",
    "OPIMIZER='SGD'\n",
    "def SeriesGen(N):\n",
    "    x = torch.arange(1, N, 0.01)\n",
    "    return torch.sin(x)#+torch.cos(x)+torch.pow(x,2)\n",
    "\n",
    "\n",
    "def trainDataGen(seq, k):\n",
    "    dat = list()\n",
    "    L = len(seq)\n",
    "    for i in range(L - k - 1):\n",
    "        indat = seq[i:i + k]\n",
    "        outdat = seq[i + 1:i + k + 1]\n",
    "        dat.append((indat, outdat))\n",
    "    return dat\n",
    "\n",
    "'''数据归一化到0-1'''\n",
    "def normalization(y):\n",
    "    max_value = np.max(y)\n",
    "    min_value = np.min(y)\n",
    "    scalar = max_value - min_value\n",
    "    y = list(map(lambda x: x / scalar, y))\n",
    "    return  y\n",
    "\n",
    "'''产生数据'''\n",
    "DATA_SIZE=10\n",
    "y = SeriesGen(DATA_SIZE).numpy()\n",
    "y=normalization(y)\n",
    "dat = trainDataGen(y, TIME_STEP)\n",
    "\n",
    "'''训练数据和测试数据设置'''\n",
    "train_size = int(len(dat) * TRAIN_RATE)\n",
    "test_size = len(dat) - train_size\n",
    "train_y = dat[:train_size]\n",
    "test_y = dat[train_size:]\n",
    "\n",
    "\n",
    "# test_x = range(int(len(dat) * TRAIN_RATE), len(dat))\n",
    "\n",
    "\n",
    "\n",
    "class LSTMpred(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(LSTMpred, self).__init__()\n",
    "        self.input_dim = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, 1)\n",
    "    #     self.hidden = self.init_hidden()\n",
    "    #\n",
    "    # def init_hidden(self):\n",
    "    #     return (Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "    #             Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, seq):\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            seq.view(len(seq), 1, -1))\n",
    "        outdat = self.hidden2out(lstm_out.view(len(seq), -1))\n",
    "        return outdat\n",
    "\n",
    "\n",
    "def ToVariable(x):\n",
    "    tmp = torch.FloatTensor(x)\n",
    "    return Variable(tmp)\n",
    "\n",
    "\n",
    "def model_training():\n",
    "    print('Start training...\\n')\n",
    "\n",
    "    model = LSTMpred(INPUT_SIZE, HIDDEN_SIZE).cuda()\n",
    "\n",
    "    loss_function = nn.MSELoss()\n",
    "    if OPIMIZER=='Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        for seq, outs in train_y:\n",
    "            seq = ToVariable(seq).cuda()\n",
    "            outs = ToVariable(outs).cuda()\n",
    "            outs=torch.unsqueeze(outs,1)\n",
    "            # outs = torch.from_numpy(np.array([outs]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # model.hidden = model.init_hidden()\n",
    "\n",
    "            modout = model(seq)\n",
    "\n",
    "            loss = loss_function(modout, outs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch:',epoch,'loss',loss.cpu().data.numpy())\n",
    "\n",
    "        torch.save(model,'model_rnn.pkl')\n",
    "\n",
    "def model_test():\n",
    "    model=torch.load('model_rnn.pkl')\n",
    "    predDat = []\n",
    "    for seq, trueVal in dat:\n",
    "        seq = ToVariable(seq)\n",
    "        trueVal = ToVariable(trueVal)\n",
    "        predDat.append(model(seq)[-1].data.numpy()[0])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.title(OPIMIZER)\n",
    "    plt.plot(y)\n",
    "    plt.plot(predDat)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.set_device(0)\n",
    "    model_training()\n",
    "\n",
    "    model_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
